{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "driver.get('https://m.naver.com')\n",
    "time.sleep(1)\n",
    "\n",
    "element = driver.find_element(By.ID, 'MM_SEARCH_FAKE')\n",
    "\n",
    "element.click()\n",
    "time.sleep(1)\n",
    "\n",
    "element2 = driver.find_element(By.ID, 'query')\n",
    "\n",
    "element2.send_keys('구로디지털역 맛집')\n",
    "search_element = driver.find_element(By.XPATH, '//*[@id=\"sch_w\"]/div/form/button')\n",
    "search_element.click()\n",
    "time.sleep(3)\n",
    "\n",
    "map_element = driver.find_element(By.XPATH, '//*[@id=\"place-main-section-root\"]/div/div[2]/a')\n",
    "map_element.click()\n",
    "time.sleep(2)\n",
    "\n",
    "list_button = driver.find_element(By.XPATH, '//*[@id=\"_place_portal_root\"]/div/a')\n",
    "list_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "store = driver.find_element(By.CLASS_NAME, 'UEzoS')\n",
    "store.click()\n",
    "time.sleep(2)\n",
    "\n",
    "review_button = driver.find_element(By.CLASS_NAME, 'veBoZ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "review_tag = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "review_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "more_button = driver.find_element(By.CLASS_NAME, 'fvwqf')\n",
    "for i in range(2):\n",
    "    more_button.click()\n",
    "    # 1초 대기\n",
    "    time.sleep(1)\n",
    "\n",
    "soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# li 태그중 class가 owAeM인 모든 태그를 찾는다.\n",
    "\n",
    "li_list = soup.find_all('li', attrs={\n",
    "    'class' : 'owAeM'\n",
    "})\n",
    "\n",
    "reviews = []\n",
    "\n",
    "i = 1\n",
    "\n",
    "for li_data in li_list:\n",
    "    review_data = li_data.find('span', attrs={\n",
    "    'class' : 'zPfVt'\n",
    "    }).get_text()\n",
    "\n",
    "    reviews.append(review_data)\n",
    "\n",
    "    # 프로필 사진을 제외한 구역의 이미지를 가져오기 위해 filter\n",
    "\n",
    "    div_data = li_data.find('div', attrs={\n",
    "        'class' : 'VAvOk'\n",
    "    })\n",
    "\n",
    "\n",
    "    # list_data에서 이미지 주소를 모두 출력\n",
    "\n",
    "    img_list = div_data.find_all('img')\n",
    "\n",
    "\n",
    "\n",
    "    for img in img_list:\n",
    "        file_name = f'review_{i}.png'\n",
    "        save_path = './img/'\n",
    "        image_save(img_path= img['src'], save_path= save_path, file_name= file_name)\n",
    "        i += 1\n",
    "\n",
    "data = pd.Series(reviews)\n",
    "\n",
    "data.to_csv('reviews.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
